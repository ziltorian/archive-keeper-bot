import os
import glob
import re
import asyncio
from difflib import SequenceMatcher
import json
from aiogram import Bot, Dispatcher, types
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram import Router
from openai import OpenAI
from dotenv import load_dotenv
from logger_setup import logger as log

# –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
from config import (
    ARCHIVE_KEEPER_PROMPT,
    OPENAI_STOPWORDS,
    SEARCH_STOPWORDS,
    MAX_USER_INPUT_LENGTH,
    MAX_SNIPPET_LENGTH,
    MAX_DEFINITIONS_FROM_GLOSSARY,
    MAX_DEFINITIONS_FROM_ARCHIVE,
    HELP_COMMAND,
    GLOSSARY_TRIGGER_WORD
)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ .env
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã
if not TELEGRAM_TOKEN or not OPENAI_API_KEY:
    raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç TELEGRAM_BOT_TOKEN –∏–ª–∏ OPENAI_API_KEY –≤ .env —Ñ–∞–π–ª–µ.")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–æ—Ç–∞
bot = Bot(token=TELEGRAM_TOKEN)
log.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞")
storage = MemoryStorage()  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è, –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è
log.info("–ß—Ç–æ-—Ç–æ —Ç–∞–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
dp = Dispatcher(storage=storage)
log.info("–î–∏—Å–ø–∞—Ç—á–µ—Ä...")
router = Router()
dp.include_router(router)

try:
    client = OpenAI()
    client.models.list()  # –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π = –ø—Ä–æ–≤–µ—Ä–∫–∞ API
    log.info("‚úÖ OpenAI API —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
except Exception as e:
    log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI: {e}")
    client = None  # –æ—Ç–∫–ª—é—á–∞–µ–º OpenAI, —á—Ç–æ–±—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–ª—å—à–µ


# –ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è
with open("archive/–ì–ª–æ—Å—Å–∞—Ä–∏–π_–•—Ä–∞–Ω–∏—Ç–µ–ª—è_–ê—Ä—Ö–∏–≤–∞.json", "r", encoding="utf-8") as f:
    log.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: –ì–ª–æ—Å—Å–∞—Ä–∏–π_–•—Ä–∞–Ω–∏—Ç–µ–ª—è_–ê—Ä—Ö–∏–≤–∞.json")
    glossary = json.load(f)

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∞—Ä—Ö–∏–≤–∞ {–∏–º—è —Ñ–∞–π–ª–∞: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ}
archive_data = {}
for filepath in glob.glob("archive/*.txt"):
    filename = os.path.basename(filepath)
    log.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {filename}")
    with open(filepath, "r", encoding="utf-8") as f:
        archive_data[filename] = f.read()


# –°–ø–∏—Å–æ–∫ "—Å–ª—É–∂–µ–±–Ω—ã—Ö" —Å–ª–æ–≤ ‚Äî –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ


def search_archive(question, only_glossary=False):
    log.info(f"üîç –ü–æ–∏—Å–∫ –Ω–∞—á–∞—Ç. –≤–æ–ø—Ä–æ—Å: {question}")
    """
    –ò—â–µ—Ç –æ—Ç–≤–µ—Ç –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏ (–ø–æ —Å–º—ã—Å–ª—É) –∏–ª–∏ –≤ –∞—Ä—Ö–∏–≤–Ω—ã—Ö .txt-—Ñ–∞–π–ª–∞—Ö.
    –°–Ω–∞—á–∞–ª–∞ –æ—Ç–±–∏—Ä–∞—é—Ç—Å—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ—Ä–Ω—é —Å–ª–æ–≤–∞,
    –∑–∞—Ç–µ–º ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∫–ª—é—á–∞–º–∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è.
    """

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞, –∏—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ
    raw_words = re.findall(r'\b\w+\b', question.lower())
    raw_words = [w for w in raw_words if w not in SEARCH_STOPWORDS and len(w) >= 4]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–≤—É—Ö—Å–ª–æ–≤–Ω—ã—Ö —Ñ—Ä–∞–∑ (n-–≥—Ä–∞–º–º)
    two_word_phrases = [
        f"{raw_words[i]} {raw_words[i + 1]}" for i in range(len(raw_words) - 1)
    ]

    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 4 —Å–∏–º–≤–æ–ª–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∫–æ—Ä–µ–Ω—å)
    search_roots = [word[:4] for word in raw_words]
    log.info(f"üî§ –°–ª–æ–≤–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {raw_words}")
    log.info(f"ü™µ –ö–æ—Ä–Ω–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞: {search_roots}")
    log.info(f"üî§ –î–≤—É—Ö—Å–ª–æ–≤–Ω—ã–µ —Ñ—Ä–∞–∑—ã: {two_word_phrases}")

    candidate_matches = []

    # –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ñ—Ä–∞–∑–∞–º
    for phrase in two_word_phrases:
        for key in glossary.keys():
            similarity = SequenceMatcher(None, phrase.lower(), key.lower()).ratio()
            if similarity >= 0.85:
                log.info(f"ü§ù –ü–æ—Ö–æ–∂–∞—è —Ñ—Ä–∞–∑–∞: '{phrase}' ‚âà '{key}' ‚Üí {similarity:.2f}")
                candidate_matches.append((similarity, f"{key}: {glossary[key]}"))
                log.info(f"–î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—É: {key}: {glossary[key]}")
                break

    # –ò—â–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏
    for word, root in zip(raw_words, search_roots):
        for key in glossary.keys():
            key_lower = key.lower()
            if key_lower.startswith(root) or root.startswith(key_lower[:4]):

                # –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: –æ—Ü–µ–Ω–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ–ª–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏ –∫–ª—é—á–∞
                similarity = SequenceMatcher(None, word, key_lower).ratio()
                if similarity >= 0.8:
                    log.info(f"üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: '{word}' ‚Üî '{key}' ‚Üí {similarity:.2f}")
                    candidate_matches.append((similarity, f"{key}: {glossary[key]}"))
                    log.info(f"–î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {key}: {glossary[key]}")
                    # break  # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–æ

    # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–µ
    if candidate_matches:
        log.info("‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏.")
        candidate_matches.sort(reverse=True)  # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        seen = set()  # –ë—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø–æ–≤—Ç–æ—Ä–æ–≤
        definitions = []  # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Å–æ–±–µ—Ä—ë–º
        for _, definition in candidate_matches:
            if definition not in seen:
                definitions.append(definition)
                seen.add(definition)
        definitions = definitions[:MAX_DEFINITIONS_FROM_GLOSSARY]  # –∏–ª–∏ 5
        log.info(f"‚úÖ –ò—Ç–æ–≥: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(definitions)} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ(–π) –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è.")
        return "\n".join(definitions)

    if only_glossary:
        return ""  # –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏, –Ω–µ –∏—â–µ–º –≤ –∞—Ä—Ö–∏–≤–µ

    # –ü–µ—Ä–µ—Ö–æ–¥ –∫ .txt-—Ñ–∞–π–ª–∞–º –∞—Ä—Ö–∏–≤–∞, –µ—Å–ª–∏ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
    log.warning("‚ö†Ô∏è –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏. –ü–µ—Ä–µ—Ö–æ–¥ –∫ –∞—Ä—Ö–∏–≤—É...")
    context_snippets = []
    for _, text in archive_data.items():
        if len(context_snippets) >= MAX_DEFINITIONS_FROM_ARCHIVE:
            break  # –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ 3-—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π

        # –î–µ–ª–∏–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—É—á–∏—Ç—ã–≤–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã)
        sentences = re.split(r'\n{2,}|\r\n{2,}|(?<=[.!?])\s+(?=[–ê-–ØA-Z])', text)

        for idx, sentence in enumerate(sentences):
            if any(root in sentence.lower() for root in search_roots):
                # –ë–µ—Ä—ë–º —ç—Ç–æ –∏ —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                snippet_group = sentences[idx:idx + 2]
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                snippet = " ".join(s.strip() for s in snippet_group)
                snippet = snippet[:MAX_SNIPPET_LENGTH]  # ‚¨ÖÔ∏è –≤–æ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
                context_snippets.append(snippet)
                log.info(f"üìÇ –ê—Ä—Ö–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã, –Ω–∞–π–¥–µ–Ω–æ: {snippet}.")
                break  # –ø–æ –æ–¥–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É –Ω–∞ —Ñ–∞–π–ª
    log.info(f"üìÇ –ê—Ä—Ö–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã: –Ω–∞–π–¥–µ–Ω–æ {len(context_snippets)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.")
    return "\n".join(context_snippets)


def extract_question_part(text, max_len=300):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –≤–æ–ø—Ä–æ—Å–æ–º. –ï—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç ‚Äî –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–µ max_len —Å–∏–º–≤–æ–ª–æ–≤.
    """
    # –î–µ–ª–∏–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'(?<=[.!?])\s+', text)
    for sentence in sentences:
        if "?" in sentence:
            return sentence.strip()[:max_len]
    return text[:max_len]  # fallback: –ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤


def clean_for_openai(text):
    """
    –£–¥–∞–ª—è–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∫–æ–º–∞–Ω–¥—ã –∏ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–±–∏—Ç—å OpenAI —Å –æ–±—Ä–∞–∑–∞.
    """
    lowered = text.lower()
    for stop in OPENAI_STOPWORDS:
        if stop in lowered:
            log.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω–æ–µ —Å–ª–æ–≤–æ: {stop}")
            lowered = lowered.replace(stop, "")
    return lowered.strip()


#
# # –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
# def search_archive(question):
#     print("–ü–æ–∏—Å–∫–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞.")
#     question_lower = question.lower()
#     context_snippets = []
#
#     for _, text in archive_data.items():
#         # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
#         # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ—á–∫—É, –≤–æ–ø—Ä–æ—Å, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ, –∑–∞—Ç–µ–º –ø—Ä–æ–±–µ–ª –∏ –∑–∞–≥–ª–∞–≤–Ω—É—é –±—É–∫–≤—É –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏
#         # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∫–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ [.?!], –Ω–æ –∏–∑–±–µ–≥–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∏ –∫–∞–≤—ã—á–µ–∫
#         sentences = re.split(r'(?<=[.!?])(?<!\b[–ê-–ØA-Z]\.)(?<!\.\")\s+(?=[–ê-–ØA-Z])', text)
#
#         for idx, sentence in enumerate(sentences):
#             if question_lower in sentence.lower():
#                 # –ë–µ—Ä—ë–º —Ç–µ–∫—É—â–µ–µ –∏ 2 —Å–ª–µ–¥—É—é—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
#                 snippet_group = sentences[idx:idx + 3]
#                 snippet = " ".join(s.strip() for s in snippet_group)
#                 context_snippets.append(snippet)
#                 break  # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ —Ñ–∞–π–ª
#     print(context_snippets)
#     # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –µ–¥–∏–Ω—ã–π —Ç–µ–∫—Å—Ç
#     return "\n".join(context_snippets)


@router.message()
async def handle_message(message: types.Message):
    text = message.text
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç–∞
    if not text:
        return

    if text.startswith(HELP_COMMAND):
        await message.reply(
            "üìú –ù–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–•—Ä–∞–Ω–∏—Ç–µ–ª—å, –∫—Ç–æ —Ç–∞–∫–∞—è –ê–≥–∞—Ç–∞?¬ª\n–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—É: –ì–ª–æ—Å—Å–∞—Ä–∏–π: –∞–≥–∞—Ç–∞")
        return

    # üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–∏–¥–∞: "–ì–ª–æ—Å—Å–∞—Ä–∏–π –ê–≥–∞—Ç–∞", "–≥–ª–æ—Å—Å–∞—Ä–∏–π: —è–¥—Ä–æ"
    if text.lower().startswith(GLOSSARY_TRIGGER_WORD):
        query = text[9:].strip()
        log.info(f"üîé –ì–ª–æ—Å—Å–∞—Ä–∏–π-–∑–∞–ø—Ä–æ—Å: {query}")

        if not query:
            await message.reply("–ß—Ç–æ –∏—Å–∫–∞—Ç—å –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏?")
            log.warning(f"üîé –ì–ª–æ—Å—Å–∞—Ä–∏–π: –Ω–µ –ø–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å.")
            return

        result = search_archive(query, only_glossary=True)
        if result:
            await message.reply(result)
            log.info(f"üîé –ì–ª–æ—Å—Å–∞—Ä–∏–π-–æ—Ç–≤–µ—Ç: {result}")
        else:
            log.warning(f"üîé –ì–ª–æ—Å—Å–∞—Ä–∏–π: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞—à–ª–∞—Å—å.")
            await message.reply("–í –≥–ª–æ—Å—Å–∞—Ä–∏–∏ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É.")
        return

    if "?" in text:  # –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫

        # 1. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∞—Ä—Ö–∏–≤–µ
        relevant_info = search_archive(text)

        # üîç Fallback, –µ—Å–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        if not relevant_info:
            relevant_info = "–ê—Ä—Ö–∏–≤ –±—ã–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω."

        # 2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è ChatGPT
        log.info("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è ChatGPT...")
        messages = [
            {"role": "system", "content": ARCHIVE_KEEPER_PROMPT}
        ]
        if relevant_info:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∞—Ä—Ö–∏–≤–∞ –∫–∞–∫ —á–∞—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –∏–ª–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            log.info("–î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∞—Ä—Ö–∏–≤–∞ –∫–∞–∫ —á–∞—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã:")
            messages.append({
                "role": "system",
                "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∞—Ä—Ö–∏–≤–∞: {relevant_info}"
            })

        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç "–æ–ø–∞—Å–Ω—ã—Ö" —Å–ª–æ–≤
        cleaned_text = clean_for_openai(text)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã, –µ—Å–ª–∏ –¥–ª–∏–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å
        if len(cleaned_text) > MAX_USER_INPUT_LENGTH and "?" in cleaned_text:
            log.info("‚úÇÔ∏è –ó–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω—ã–π. –û–±—Ä–µ–∑–∞–µ–º –¥–æ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.")
            cleaned_text = extract_question_part(cleaned_text)

        messages.append({"role": "user", "content": cleaned_text})

        # 3. –ó–∞–ø—Ä–æ—Å –∫ OpenAI ChatCompletion
        log.info(f"üîÅ –ó–∞–ø—Ä–æ—Å –∫ OpenAI: {messages}")
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini-2024-07-18",  # –∏–ª–∏ "gpt-4" –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥–æ—Å—Ç—É–ø–∞
                messages=messages
            )
            log.info(f"ü§ñ –û—Ç–≤–µ—Ç –æ—Ç OpenAI: {response}")
        except Exception as e:
            await message.reply("–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫...")
            log.error(f"‚ö†Ô∏è OpenAI API error: {e}")
            return

        # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ —á–∞—Ç
        answer = response.choices[0].message.content
        await message.reply(answer)

        # 5. –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å
        log.info(f"üí¨ Q: {text}\nüí¨ A: {answer}\n{'-' * 40}")
    # –ï—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –Ω–µ—Ç –≤–æ–ø—Ä–æ—Å–∞, –±–æ—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥—É—é –ª–æ–≥–∏–∫—É –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)


async def main():
    await dp.start_polling(bot)

if __name__ == "__main__":
    log.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    try:
        asyncio.run(main())
    except Exception as e:
        import traceback
        log.error("‚ÄºÔ∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:")
        traceback.print_exc()
